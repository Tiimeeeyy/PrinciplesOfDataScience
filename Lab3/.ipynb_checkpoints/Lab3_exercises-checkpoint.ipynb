{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ab7e4d3-00aa-4db2-af93-6becd3d20723",
   "metadata": {},
   "source": [
    "# KEN1435 - Principles of Data Science | Lab 3: The Normal Approximation for Data\n",
    "\n",
    "First we load the necessary python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7af88abc-70fb-4f88-86cd-19c8bb506211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates # you can use this to customize your figures for exercises 9, 10 and 14\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from palmerpenguins import load_penguins\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6c68c-d3cf-4587-b150-b07605cfbb48",
   "metadata": {},
   "source": [
    "Let's start off with a normal approximation for the bill length of the Palmer penguins. Recall that we already displayed the mean absolute deviation of this quantity in exercise 7 of the previous lab. First, we load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad6cb4c3-f349-4315-941f-b36e4a9824c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = load_penguins()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fca7d2-3e41-48cb-8402-a34289cf63fb",
   "metadata": {},
   "source": [
    "1. Calculate the average and standard deviations of the bill length for each species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7515c276-767f-41d2-bf96-a1db2e66265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0770ab7-3a1e-420d-82fd-edb773990830",
   "metadata": {},
   "source": [
    "2. Calculate the standardized bill lengths for all penguins, taking into account which species they belong to. Save your result in a column named `bill_sd` in the `penguins` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c4c1f-b643-4607-9862-ecfdf11b1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0b333-9d36-4a46-bdde-6e82a6554685",
   "metadata": {},
   "source": [
    "3. Plot histograms of the column `bill_sd`, such that each species is displayed in its own panel. Also calculate which fraction of the penguins fall within two standard deviations from the mean. Based on the output, could you use a normal approximation for the bill length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b924ee-d817-438c-8694-3a362b7d4e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec6b8b6-9a1b-427d-a754-db528e4dbeae",
   "metadata": {},
   "source": [
    "***Answer:*** *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215c063b-f3e9-4f26-9723-a7b5a512ce3f",
   "metadata": {},
   "source": [
    "## COVID-19: sentiment in Maastricht\n",
    "Let us now consider another data set that is extracted from social media. It covers a collection of users on social media that were retrieved based on posts that referenced a list of keywords related to the COVID-19 pandemic. Among all messages acquired with these keywords, those users were extacted that indicated their location was a city in the Netherlands. Specifically, we take a look at the accounts that specified \"Maastricht\" as their location.\n",
    "\n",
    "First, we load a data frame with general information with regards to the accounts. As the file uses tab as a seperator, we specify this in while loading the file by `sep=\"\\t\"`. As the index of the file is contained in the first row of the file, we include `index_col=0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfd25ad-61b3-4396-a4e5-3aa408d8bb7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>followers</th>\n",
       "      <th>friends</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>043u0001</th>\n",
       "      <td>733</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>043u0002</th>\n",
       "      <td>3882</td>\n",
       "      <td>539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>043u0003</th>\n",
       "      <td>92</td>\n",
       "      <td>647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>043u0004</th>\n",
       "      <td>87</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>043u0005</th>\n",
       "      <td>3271</td>\n",
       "      <td>3021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          followers  friends\n",
       "user_id                     \n",
       "043u0001        733      894\n",
       "043u0002       3882      539\n",
       "043u0003         92      647\n",
       "043u0004         87      122\n",
       "043u0005       3271     3021"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uinfo = pd.read_csv(\"users_maastricht.tsv\", sep=\"\\t\", index_col=0)\n",
    "uinfo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f41605-17a5-47a6-847b-03bdd89e748d",
   "metadata": {},
   "source": [
    "Let's plot the distributions of this data.\n",
    "\n",
    "4. Visualize friends and followers distributions and plot the corresponding histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30347428-37bb-4060-835e-ee2baf9c4803",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41f4dbc-c330-4ce9-86b3-487ecdf7b8bf",
   "metadata": {},
   "source": [
    "As you can see from the figures, this is far from a normal distribution. Lets first look at the minimum and maximum observations for the friends and followers.\n",
    "\n",
    "5. Determine the minimum and the maximum counts for both the friend and follower distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3816765-cf8b-4b6b-81d8-84d34ca136cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce57b41-cf4e-4002-8615-d4fc8fe89a30",
   "metadata": {},
   "source": [
    "As you can see, the values span multiple orders of magnitude, with only a few very large observations. This is a clear indication of a heavy tail distribution. However, there is another way that we can try to perform a normal approximation for this data.\n",
    "\n",
    "In this alternative, we first calculate the logarithm of all observations (with `np.log10`, see documentation [here](https://numpy.org/doc/stable/reference/generated/numpy.log10.html)) and use those values for a normalization.\n",
    "\n",
    "6. Plot the normal approximation of the logarithm of the friend and follower data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81460add-7120-4a2d-975a-074bad56d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977f4d3-01c7-45ea-a8c0-b69cf4d66c53",
   "metadata": {},
   "source": [
    "7. Which fraction of the logarithmic scale observations are within two standard deviations from the mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5dc692-d942-446e-ae0d-8484e913f896",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676b3a5-01e6-42f6-9fa9-e2747cdd9c05",
   "metadata": {},
   "source": [
    "The fact that calculating the logarithm of the data made our observations fall within a good normal approximation, tells us that the data can be approximated using a log-normal distribution.\n",
    "\n",
    "Next, we move on to the messages placed by the users. For this, we load the next data file `tweets_maastricht.tsv`. In this file, we have information about which user posted the tweet and at which time, in addition to the sentiment scores for the valence, arousal and dominance dimensions. We set the column `tweet_id` as the index. Moreover, the file is again tab-separated, so we use `sep=\"\\t\"`. Finally, to process the dates at which the tweets are posted, we also specify a function that converts them to the correct timestamps, namely `pd.to_datetime`. Note that loading this dataframe might take some time as a result of this conversion to datetime objects. Therefore, we include the `%%time` command, which allows us to track how long it takes to run a particular cell of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7baf4f-503a-4764-aae0-b3f73c401086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sent = pd.read_csv(\"tweets_maastricht.tsv\", sep=\"\\t\", index_col=\"tweet_id\", converters={\"created_at\": pd.to_datetime})\n",
    "sent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df649ac7-c2e2-4dec-b48b-52f60900721c",
   "metadata": {},
   "source": [
    "Let's take a closer look at the `created_at` column. Obviously, this is `Series`-object, which consists of `datetime64` objects. Note that these objects are in UTC (Universal Coordinated Time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be123c6-8c2a-4d29-af6a-eae17310dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.created_at"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592a882e-7890-48cd-a95b-4fe54cfdd0f2",
   "metadata": {},
   "source": [
    "In the case that we have a `Series` of datetime objects, we can use the `.dt` accessor (see documentation [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/basics.html#dt-accessor)) to extract properties of each timestamp across the entire `Series`. For instance, we can determine the date at which tweet is posted as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68d87f-a1a2-45bb-a515-30c36093b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.created_at.dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b647e-4759-489a-b2c6-8d6abbe4de61",
   "metadata": {},
   "source": [
    "8. Construct a time-series that contains the daily number of tweets contained in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c73eb-4e46-4ba7-96e9-dcc3ab6fd434",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c865b6-2224-4d68-b3f2-6ab0ca37948d",
   "metadata": {},
   "source": [
    "9. Determine the time series of daily number of tweets in the data set (save the result in the variable `ts`) and plot the number of daily tweets starting from January 1st 2017 up to the latest day in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84a5c1d-e8b4-4f37-9d55-b1158860386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5d010f-1fcf-4322-ad09-9e96b661a9a1",
   "metadata": {},
   "source": [
    "It is clear that the majority of the tweets are observed in the last months in the timespan of the data. Let's also look at the weekly average number of  tweets. We can do this by using `.rolling` (see documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rolling.html)), by using the window 7. After we use `.rolling(7)`, we can then do a calculation on the rolling window, e.g., by executing `mean` in our case. We get the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe65fc-5665-43f6-9f15-e29932a9fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.rolling(7).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5364bf4-5d8a-4479-b8f8-ae62d7bdedea",
   "metadata": {},
   "source": [
    "As you can see, the first observations are `NaN`-values, as there are not enough observations before that obervation to average over seven observations.\n",
    "\n",
    "10. Include the rolling weekly average in the plot that you made in the previous exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef91d334-6c3d-4782-969f-870f1ad206bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd8bd7d-b5ad-4460-9407-596913b6fbe7",
   "metadata": {},
   "source": [
    "Looking at this weekly average, we see a slight increase in number of tweets over time. Let's take a close look at this. First, we will determine the times of the first and last tweets per account in the dataset and store this information in the variables `first_tweet` and `last_tweet`, respectively.\n",
    "\n",
    "11. Determine the time of the first and last tweet of all accounts in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949dc2b2-dd6f-4eb7-a4f9-2a075904504b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0fd3e7-0d61-4826-b77a-73221129859f",
   "metadata": {},
   "source": [
    "12. Calculate the difference in days between the two, so we can sort the accounts based on length, use the `.dt` accessor to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7882e9bc-111c-41d3-a865-31ca5c695e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee02941-7d06-45a6-80df-c4c21e3726a9",
   "metadata": {},
   "source": [
    "With this days difference between first and last tweet, we can sort the accounts based on the length of their timeline for visualization purposes. To do so, we have to determine the rank of the observations and subsequently sort these ranks. Save the ordering in the variable `order`.\n",
    "\n",
    "13. Sort the accounts based on the number of days in their timeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1432981-bf81-43b3-833c-24572314a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3b764-21c5-4ba2-8a5a-74cb6fc9f443",
   "metadata": {},
   "source": [
    "Let's visualize these quantities by drawing horizontal lines that connect the first and last tweet timings.\n",
    "\n",
    "14. Visualize the lenghts of each timeline of the accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f29f8b-76c1-44ce-b654-85c7e826ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac3d119-538e-4cb5-bb44-33750df9a67b",
   "metadata": {},
   "source": [
    "As you can see in the figure above, some of the time-series of message for specific users are rather short. This is a result of the fact that we could only obtain a specific number of tweets per user, so if a user is more active, we will have a shorter history for that user. This also directly explains why the number of tweets steadily increases as we move closer to the end of the observed times.\n",
    "\n",
    "## Sentiment distribution\n",
    "Next, we turn our attention to the sentiment data. As we saw in the first overview of the data, there are several `NaN`-values in the data. First, we want to investigate how many observations have `NaN`-values.\n",
    "\n",
    "15. Save the names of the sentiment columns in the variable `sent_cols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7326d72-b32a-4d63-9370-29344e45fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148c979-3729-4580-9d51-314e4d4c1788",
   "metadata": {},
   "source": [
    "16. Determine how many observations are `NaN` in the three sentiment columns in the data frame `sent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2257737-c983-4b76-8b12-4afb2375ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6c6743-a83c-4dc7-9080-851471958a99",
   "metadata": {},
   "source": [
    "Next, let's first take a look at the distributions of the observations by visualizaing the distributions\n",
    "\n",
    "17. Display the distriubtion of the sentiment values in a histogram for each of the dimensions. Bin the observations from `1` to `9` in bins of a width of `0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ebecdb-28c2-4086-a42e-0e64e105b368",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ac3c9-7dd6-4e9f-8fb5-2184e4a933aa",
   "metadata": {},
   "source": [
    "Let's explore whether the normal approximation works for the sentiment data. First, we have to normalize the data.\n",
    "\n",
    "18. Calculate how many standard deviations away from the mean each observation is for each sentiment dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5175e3-eaee-42a9-a606-80bd1b6d90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793050cb-521d-47fb-bc1d-4118d23a6aac",
   "metadata": {},
   "source": [
    "Now that we have the deviations from the mean, we can determine how many observations lie within two standard deviations of the mean.\n",
    "\n",
    "19. Determine which fraction of the observations falls within two standard deviations from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b676335-cc74-45e2-b553-f3091b038d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098e2505-d6f5-4fe4-8ab6-6d3310765cc0",
   "metadata": {},
   "source": [
    "Based on these observations, would you say that the normal approximation for the sentiment data is appropriate?\n",
    "\n",
    "***Answer:*** *YOUR ANSWER HERE*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29144e68-5d9d-4352-8de8-dc04f6eca80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
